# The Cross Validators Research Report ------------------------------------

# Talis Tebecis h12135076
# Ema Ivanova h11900690
# Karin Nagamine h12131854
# Stephan Gavric h12025079


# Setup -------------------------------------------------------------------

# clear environment
rm(list=ls())

# install packages
pacman::p_load(here,
               tidyverse,
               BMS,
               xgboost,
               caret,
               readxl,
               sf,
               ggplot2,
               ggpubr)

# We use the original 1995-2005 data from the Crespo Cuaresma and Feldkircher (2013) study, "SPATIAL FILTERING, MODEL UNCERTAINTY AND THE SPEED OF INCOME CONVERGENCE IN EUROPE"
# The original authors use a spatial filtering model, but we use a simple Bayesian Model Averaging (BMA) process without spatial filters

# read in data from Crespo Cuaresma and Feldkircher
orig_data <- read.delim(here("data", "variables_crespo.txt"), header = TRUE, sep = " ", dec = ".")
head(orig_data)

# Import shape files
temp <- tempfile(fileext = ".zip")
#download.file("http://ec.europa.eu/eurostat/cache/GISCO/distribution/v2/nuts/download/ref-nuts-2013-03m.shp.zip", temp)
outDir <- "./data"
#unzip(temp, exdir=outDir)
list.files(path="./data", pattern = '*.shp')
unzip("./data/NUTS_RG_03M_2013_4326_LEVL_2.shp.zip", exdir=outDir)
shp <- st_read(dsn = "./data", layer ="NUTS_RG_03M_2013_4326_LEVL_2") %>% 
        rename(region = NUTS_ID)
# remove overseas
overseas <- c("FRA1", "FRA2", "FRA3", "FRA4", "FRZZ", "FRA5", "PT20", "PT30", "PTZZ", "ES70", "ESZZ")
shp <- shp[! shp$region %in% overseas, ]
plot(st_geometry(shp))

# BMA ---------------------------------------------------------------------
# Use Bayesian model averaging to determine the key variables in determining cross-region growth for 1995-2005

# note, the original estimation is based on 15 million posterior draws (iterNr=14e06) and 5 mil burn-ins (burnNr=5e06)
# we use 200k burn ins and 600k iterations due to the lower complexity - not accounting  for spatial filtering
# model still returns correlation with analytical results of 0.993, which iis very high
model <- bms(orig_data, burn=200000, iter=600000, g="BRIC", mprior="uniform", nmodel=2000, mcmc="bd")
summary(model)

# find top models
topmods = topmodels.bma(model)
View(topmods)

#PIP
coef(model)

# Plot the prior and posterior model size distribution.
plotModelsize(model)

# Plot posterior model probabilities (MCMC vs Exact)
plotConv(model)

# Plot density for different variables.
par(mfrow=c(3,2))
density(model,"Capital") #Capital city
density(model,"GDPCAP0") #Initial real GDP per capita (in logs)
density(model,"ShSH") #Share of higher educated in working age population
density(model,"AccessRoad") #Potential accessibility road
density(model,"POPDENS0") #Initial population density
density(model,"AirportDens") #Airport density
dev.off()
par(mfrow=c(2,2))
density(model,"AccessAir") #Potential accessibility air
density(model,"EMPDENS0") #Initial employment density
density(model,"ShCE0") #Initial share of NACE C to E (Mining, Manufacturing and Energy)
dev.off()

# Plot the signs of the different variables for the 500 best models.
image(model[1:500])

# Print the posterior expected coefficient values for the 5 best models.
beta.draws.bma(model[1:5])

# top variables (PIP >0.5)
# NOte, Airport density omited due to lack of data availability
top_vars <- c("Capital", "GDPCAP0", "ShSH", "AccessRoad", "POPDENS0", "AccessAir", "EMPDENS0", "ShCE0")


# Boosted Regression Tree -------------------------------------------------
# train a boosted regression tree model to predict GDP growth based on the top variables from the BMA process

# subset data
training <- orig_data %>% 
        select(gGDPCAP, top_vars)

# set up control for the model with 10-fold cross validation
# best model had gamma = 0, min. child  weight = 1, nrounds = 50, max. depth = 1, eta = 0.4 and colsample_bytree = 0.6
control = trainControl(method = "cv", number = 10)
boosted = caret::train(data = training, gGDPCAP ~ ., method = "xgbTree", trControl = control)

# RMSE
min(boosted$results$RMSE)^2
boosted

# extract final model
final_model = boosted$finalModel


# Predictions for recent data ---------------------------------------------
# Use the boosted regression model to make predictions of GDP growth based on more recent data

#First load in more recent data for top variables

capital <- read.csv(here("data", "capitals.csv")) %>% 
        select(NUTS_ID, NUMPOINTS) %>% 
        rename(region = NUTS_ID,
               Capital = NUMPOINTS)

# GDP per capita (2020), GDPCAP0
gdppc <- read_excel(here("data", "gdppc_2020.xlsx"),
                    range = "Sheet 1!A10:B316",
                    col_names = F) %>% 
        rename(region = ...1,
               GDPCAP0 = ...2) %>% 
        mutate(GDPCAP0 = log(as.numeric(GDPCAP0)))

#Share of higher educated in working age population (2020), ShSH
educ_attain <- read_excel(here("data", "educ_attainment_2020.xlsx"),
                    range = "Sheet 1!A13:B355",
                    col_names = F) %>% 
        rename(region = ...1,
               ShSH = ...2) %>% 
        mutate(ShSH = as.numeric(ShSH)/100)

#Potential accessibility road (2020), AccessRoad
access_road <- read.csv(here("data", "access_road.csv")) %>% 
        select(tunit_code, y_2014) %>% 
        mutate(tunit_code = str_sub(tunit_code, 1, 4)) %>% 
        group_by(tunit_code) %>% 
        summarise(AccessRoad = mean(y_2014)/100) %>% 
        rename(region = tunit_code)

#Initial population density (2020), POPDENS0
pop_dens <- read_excel(here("data", "pop_dens.xlsx"),
                    range = "Sheet 1!A10:B341",
                    col_names = F) %>% 
        rename(region = ...1,
               POPDENS0 = ...2) %>% 
        mutate(POPDENS0 = as.numeric(POPDENS0)/1000)

#Potential accessibility air (2020), AccessAir
access_air <- read.csv(here("data", "access_air.csv")) %>% 
        select(tunit_code, y_2014) %>% 
        mutate(tunit_code = str_sub(tunit_code, 1, 4)) %>% 
        group_by(tunit_code) %>% 
        summarise(AccessAir = mean(y_2014)/100) %>% 
        rename(region = tunit_code)

#Initial employment density (2020), EMPDENS0
employed <- read_excel(here("data", "employed_NUTS2.xlsx"),
                       range = "Sheet 1!A12:B273",
                       col_names = F) %>% 
        rename(region = ...1,
               employed = ...2) %>% 
        mutate(employed = as.numeric(employed))

area <- read_excel(here("data", "area.xlsx"),
                       range = "Sheet 1!A11:B383",
                       col_names = F) %>% 
        rename(region = ...1,
               area = ...2) %>% 
        mutate(area = as.numeric(area))

emp_dens <- left_join(employed, area) %>% 
        mutate(EMPDENS0 = employed/area) %>% 
        select(region, EMPDENS0)

#Initial share of NACE C to E (Mining, Manufacturing and Energy) (2020), ShCE0
NACE <- read_excel(here("data", "NACE_and_total.xlsx"),
                       range = "Sheet 1!A13:B356",
                       col_names = F) %>% 
        rename(region = ...1,
               NACE = ...2) %>% 
        mutate(NACE = as.numeric(NACE))

NACE <- read_excel(here("data", "NACE_and_total.xlsx"),
                   range = "Sheet 2!A13:B356",
                   col_names = F) %>% 
        rename(region = ...1,
               NACE = ...2) %>% 
        mutate(NACE = as.numeric(NACE))

NACE_dens <- left_join(NACE, employed) %>% 
        mutate(ShCE0 = NACE/employed) %>% 
        select(region, ShCE0)

#Combine new data
new_data <- as.data.frame(shp$region) %>% 
        rename(region = "shp$region") %>% 
        left_join(capital) %>% 
        relocate(region) %>% 
        left_join(gdppc) %>% 
        left_join(educ_attain) %>% 
        left_join(access_road) %>% 
        left_join(pop_dens) %>% 
        left_join(access_air) %>% 
        left_join(emp_dens) %>% 
        left_join(NACE_dens) %>% 
        column_to_rownames("region")


# Predictions -------------------------------------------------------------
# use boosted regression model to make predictions on new data
prediction_2020 <- predict(final_model, newdata = as.matrix(new_data))
prediction_2005 <- predict(final_model, newdata = as.matrix(training[,2:9]))

# compare the actual GDP growth to predictions for 2005 and 2020

# calculate GDP per capita growth for 2016-2020
gGDPCAP_2020 <- read.csv(here("data", "gdppc_2016-2020_2.csv")) %>% 
        rename("2016" = X2016,
               "2017" = X2017,
               "2018" = X2018,
               "2019" = X2019,
               "2020" = X2020,) %>% 
        pivot_longer(-region,
                     names_to = "year",
                     values_to = "gdppc") %>%
        mutate(gdppc = as.numeric(gdppc)) %>% 
        arrange(region, year) %>% 
        mutate(growth = (gdppc - lag(gdppc))/lag(gdppc)) %>% 
        drop_na() %>% 
        filter(year != 2016) %>% 
        group_by(region) %>% 
        summarise(gGDPCAP_2020 = mean(growth))


# combine to compare
# note, missing values in 2020 (actual) because NUTS2 regions codes changed

combine_2005 <- cbind(training, prediction_2005) %>% 
        rownames_to_column() %>% 
        mutate(region = str_to_upper(rowname)) %>% 
        select(region, gGDPCAP, prediction_2005)

combine_2020 <- cbind(new_data, prediction_2020) %>% 
        rownames_to_column() %>% 
        rename(region = rowname) %>% 
        select(region, prediction_2020) %>% 
        left_join(gGDPCAP_2020) %>% 
        na.omit()

visuals <- shp %>% 
        left_join(combine_2005) %>% 
        left_join(combine_2020) %>% 
        mutate(error_2005 = abs(gGDPCAP-prediction_2005),
               error_2020 = abs(gGDPCAP_2020-prediction_2020))

#Calculate RMSE of predictions
RMSE_2005 <- sqrt(sum(visuals$error_2005^2, na.rm = T)) # 0.1000398
RMSE_2020 <- sqrt(sum(visuals$error_2020^2, na.rm = T)) # 0.3024585

# Visualizations ----------------------------------------------------------

# 2005 comparison
actual_2005 <- ggplot(data = visuals) +
        geom_sf(aes(fill = gGDPCAP), color = "grey") +
        scale_fill_viridis_c(option = "D", na.value="white", direction = -1, limits = c(-0.02, 0.08)) +
        labs(fill = "GDP Growth") +
        theme_minimal()

predict_2005 <- ggplot(data = visuals) +
        geom_sf(aes(fill = prediction_2005), color = "grey") +
        scale_fill_viridis_c(option = "D", na.value="white", direction = -1, limits = c(-0.02, 0.08)) +
        labs(fill = "GDP Growth") +
        theme_minimal()

error_2005 <- ggplot(data = visuals) +
        geom_sf(aes(fill = error_2005), color = "grey") +
        scale_fill_viridis_c(option = "A", na.value="white", direction = -1, limits = c(0, 0.07)) +
        labs(fill = "Error") +
        theme_minimal()

ggarrange(actual_2005, predict_2005, error_2005,
          labels = c("Actual GDP growth 2005",
                     "Predicted GDP growth 2005",
                     "Prediction error 2005"),
          ncol = 2, nrow = 2)

# 2020 comparison
actual_2020 <- ggplot(data = visuals) +
        geom_sf(aes(fill = gGDPCAP_2020), color = "grey") +
        scale_fill_viridis_c(option = "D", na.value="white", direction = -1, limits = c(-0.02, 0.08)) +
        labs(fill = "GDP Growth") +
        theme_minimal()

predict_2020 <- ggplot(data = visuals) +
        geom_sf(aes(fill = prediction_2020), color = "grey") +
        scale_fill_viridis_c(option = "D", na.value="white", direction = -1, limits = c(-0.02, 0.08)) +
        labs(fill = "GDP Growth") +
        theme_minimal()

error_2020 <- ggplot(data = visuals) +
        geom_sf(aes(fill = error_2020), color = "grey") +
        scale_fill_viridis_c(option = "A", na.value="white", direction = -1, limits = c(0, 0.07)) +
        labs(fill = "Error") +
        theme_minimal()

ggarrange(actual_2020, predict_2020, error_2020,
          labels = c("Actual GDP growth 2020",
                     "Predicted GDP growth 2020",
                     "Prediction error 2020"),
          ncol = 2, nrow = 2)

# comparison of prediction errors
ggarrange(error_2005, error_2020,
          labels = c("Actual GDP growth 2005",
                     "Predicted GDP growth 2005",
                     "Prediction error 2005"),
          ncol = 2, nrow = 1)
